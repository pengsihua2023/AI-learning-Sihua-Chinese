## 深度学习基础知识：常用激活函数简介

### Sigmoid激活函数
Sigmoid激活函数是一种常用的非线性激活函数，广泛应用于神经网络中，尤其是在二分类问题中。以下是其简介：

### 定义
Sigmoid函数的数学表达式为：
<img width="278" height="103" alt="image" src="https://github.com/user-attachments/assets/3880b02e-f4a3-450d-9339-8cdefcd9dc63" />  
  
其中，x 是输入，e是自然对数的底（约2.718）。

### 特点
1. **输出范围**：Sigmoid函数将任意实数输入映射到 (0, 1) 区间，适合表示概率或用于二分类任务。
2. **非线性**：通过引入非线性，Sigmoid帮助神经网络建模复杂的非线性关系。
3. **平滑性**：函数连续且可导，适合梯度下降优化。

### 优点
- 输出值在0到1之间，适合需要概率输出的场景，如逻辑回归或二分类神经网络。
<img width="713" height="60" alt="image" src="https://github.com/user-attachments/assets/410e7b23-5e8b-4640-92f1-081112b4b326" />
  

### 缺点
1. **梯度消失**：在输入值较大或较小时，Sigmoid的梯度接近于0，导致深层网络训练时梯度消失问题，影响学习效率。
2. **非零中心**：输出始终为正（0到1），可能导致梯度更新不平衡，减慢收敛。
3. **计算开销**：指数运算相对复杂，计算成本较高。

### 应用场景
- **二分类问题**：常用于输出层，将模型输出转化为概率。
- **传统RNN/LSTM**：早期常用于门控机制（如LSTM的遗忘门、输入门等），不过现代网络常使用其他激活函数（如tanh或ReLU）。
- **概率输出**：当需要将输出解释为概率时，Sigmoid是自然选择。

### 替代方案
由于梯度消失等问题，现代神经网络中，ReLU、Leaky ReLU或tanh等激活函数在某些场景下更受欢迎，尤其是在深层网络中。

总结来说，Sigmoid函数因其简单性和概率解释在特定场景下仍有价值，但在深层网络中需谨慎使用，以避免梯度消失等问题。
