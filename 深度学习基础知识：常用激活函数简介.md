## 深度学习基础知识：常用激活函数简介

## Sigmoid激活函数
Sigmoid激活函数是一种常用的非线性激活函数，广泛应用于神经网络中，尤其是在二分类问题中。以下是其简介：

### 定义
Sigmoid函数的数学表达式为：
<img width="278" height="103" alt="image" src="https://github.com/user-attachments/assets/3880b02e-f4a3-450d-9339-8cdefcd9dc63" />  
  
其中，x 是输入，e是自然对数的底（约2.718）。

### 特点
1. **输出范围**：Sigmoid函数将任意实数输入映射到 (0, 1) 区间，适合表示概率或用于二分类任务。
2. **非线性**：通过引入非线性，Sigmoid帮助神经网络建模复杂的非线性关系。
3. **平滑性**：函数连续且可导，适合梯度下降优化。

### 优点
- 输出值在0到1之间，适合需要概率输出的场景，如逻辑回归或二分类神经网络。
<img width="713" height="60" alt="image" src="https://github.com/user-attachments/assets/410e7b23-5e8b-4640-92f1-081112b4b326" />
  

### 缺点
1. **梯度消失**：在输入值较大或较小时，Sigmoid的梯度接近于0，导致深层网络训练时梯度消失问题，影响学习效率。
2. **非零中心**：输出始终为正（0到1），可能导致梯度更新不平衡，减慢收敛。
3. **计算开销**：指数运算相对复杂，计算成本较高。

### 应用场景
- **二分类问题**：常用于输出层，将模型输出转化为概率。
- **传统RNN/LSTM**：早期常用于门控机制（如LSTM的遗忘门、输入门等），不过现代网络常使用其他激活函数（如tanh或ReLU）。
- **概率输出**：当需要将输出解释为概率时，Sigmoid是自然选择。

### 替代方案
由于梯度消失等问题，现代神经网络中，ReLU、Leaky ReLU或tanh等激活函数在某些场景下更受欢迎，尤其是在深层网络中。

总结来说，Sigmoid函数因其简单性和概率解释在特定场景下仍有价值，但在深层网络中需谨慎使用，以避免梯度消失等问题。

## ReLU激活函数
ReLU（Rectified Linear Unit）激活函数是现代神经网络中最常用的非线性激活函数之一，因其简单性和高效性被广泛应用于深度学习模型。以下是其简介：

### 定义
ReLU函数的数学表达式为：
f(x) = max(0, x)
即：如果输入x大于0，则输出x；否则输出0。

### 特点
1. **输出范围**：ReLU将负输入映射为0，正输入保持不变，输出范围为 [0, +infty)。
2. **非线性**：通过将负值置零，ReLU引入非线性，便于神经网络建模复杂关系。
3. **稀疏性**：ReLU会使负输入的输出为0，导致部分神经元不激活，产生稀疏表示，有助于提高计算效率和泛化能力。

### 优点
1. **缓解梯度消失**：相比Sigmoid和tanh，ReLU在正输入区域的梯度恒为1，避免了梯度消失问题，加速梯度下降收敛。
2. **计算简单**：仅涉及取最大值操作，计算效率高，适合大规模网络。
3. **稀疏激活**：负输入置零导致部分神经元不激活，减少计算量并降低过拟合风险。

### 缺点
1. **死神经元问题**（Dying ReLU）：当输入始终为负时，神经元的输出和梯度均为0，导致该神经元无法更新，可能永久“死亡”。
2. **非零中心**：输出始终非负，可能导致梯度更新偏向某一方向，影响收敛速度。
3. **非光滑**：在 x=0 处不可导（数学上），尽管在实践中通过定义子梯度（如设为0）解决。

### 应用场景
- **深层神经网络**：ReLU是卷积神经网络（CNN）和全连接网络的默认激活函数，广泛用于计算机视觉和自然语言处理任务。
- **隐藏层**：常用于隐藏层，因其高效性和对梯度消失的鲁棒性。
- **加速训练**：在深层网络（如ResNet、Transformer）中，ReLU显著提升训练速度。

### 改进变体
为解决ReLU的缺点，出现了一些变体：
- **Leaky ReLU**：f(x) = max(alpha x, x)，其中 alpha 是一个小的正数（如0.01），允许负输入有小梯度，缓解死神经元问题。
- **Parametric ReLU（PReLU）**：alpha 作为可学习的参数。
- **ELU（Exponential Linear Unit）**：对负输入使用指数函数，平滑过渡并具有零中心特性。
- **GELU（Gaussian Error Linear Unit）**：结合高斯分布特性，用于Transformer模型。

### 总结
ReLU因其简单、高效和缓解梯度消失的能力，成为深度学习中的首选激活函数，尤其适合隐藏层。尽管存在死神经元等局限性，其变体（如Leaky ReLU）可进一步优化性能。在实际应用中，ReLU通常是构建深层网络的起点选择。
