## 深度学习基础知识：常用激活函数简介

## Sigmoid激活函数
Sigmoid激活函数是一种常用的非线性激活函数，广泛应用于神经网络中，尤其是在二分类问题中。以下是其简介：

### 定义
Sigmoid函数的数学表达式为：
<img width="278" height="103" alt="image" src="https://github.com/user-attachments/assets/3880b02e-f4a3-450d-9339-8cdefcd9dc63" />  
  
其中，x 是输入，e是自然对数的底（约2.718）。

### 特点
1. **输出范围**：Sigmoid函数将任意实数输入映射到 (0, 1) 区间，适合表示概率或用于二分类任务。
2. **非线性**：通过引入非线性，Sigmoid帮助神经网络建模复杂的非线性关系。
3. **平滑性**：函数连续且可导，适合梯度下降优化。

### 优点
- 输出值在0到1之间，适合需要概率输出的场景，如逻辑回归或二分类神经网络。
<img width="713" height="60" alt="image" src="https://github.com/user-attachments/assets/410e7b23-5e8b-4640-92f1-081112b4b326" />
  

### 缺点
1. **梯度消失**：在输入值较大或较小时，Sigmoid的梯度接近于0，导致深层网络训练时梯度消失问题，影响学习效率。
2. **非零中心**：输出始终为正（0到1），可能导致梯度更新不平衡，减慢收敛。
3. **计算开销**：指数运算相对复杂，计算成本较高。

### 应用场景
- **二分类问题**：常用于输出层，将模型输出转化为概率。
- **传统RNN/LSTM**：早期常用于门控机制（如LSTM的遗忘门、输入门等），不过现代网络常使用其他激活函数（如tanh或ReLU）。
- **概率输出**：当需要将输出解释为概率时，Sigmoid是自然选择。

### 替代方案
由于梯度消失等问题，现代神经网络中，ReLU、Leaky ReLU或tanh等激活函数在某些场景下更受欢迎，尤其是在深层网络中。

总结来说，Sigmoid函数因其简单性和概率解释在特定场景下仍有价值，但在深层网络中需谨慎使用，以避免梯度消失等问题。

## ReLU激活函数
ReLU（Rectified Linear Unit）激活函数是现代神经网络中最常用的非线性激活函数之一，因其简单性和高效性被广泛应用于深度学习模型。以下是其简介：

### 定义
ReLU函数的数学表达式为：
f(x) = max(0, x)
即：如果输入x大于0，则输出x；否则输出0。

### 特点
1. **输出范围**：ReLU将负输入映射为0，正输入保持不变，输出范围为 [0, +infty)。
2. **非线性**：通过将负值置零，ReLU引入非线性，便于神经网络建模复杂关系。
3. **稀疏性**：ReLU会使负输入的输出为0，导致部分神经元不激活，产生稀疏表示，有助于提高计算效率和泛化能力。

### 优点
1. **缓解梯度消失**：相比Sigmoid和tanh，ReLU在正输入区域的梯度恒为1，避免了梯度消失问题，加速梯度下降收敛。
2. **计算简单**：仅涉及取最大值操作，计算效率高，适合大规模网络。
3. **稀疏激活**：负输入置零导致部分神经元不激活，减少计算量并降低过拟合风险。

### 缺点
1. **死神经元问题**（Dying ReLU）：当输入始终为负时，神经元的输出和梯度均为0，导致该神经元无法更新，可能永久“死亡”。
2. **非零中心**：输出始终非负，可能导致梯度更新偏向某一方向，影响收敛速度。
3. **非光滑**：在 x=0 处不可导（数学上），尽管在实践中通过定义子梯度（如设为0）解决。

### 应用场景
- **深层神经网络**：ReLU是卷积神经网络（CNN）和全连接网络的默认激活函数，广泛用于计算机视觉和自然语言处理任务。
- **隐藏层**：常用于隐藏层，因其高效性和对梯度消失的鲁棒性。
- **加速训练**：在深层网络（如ResNet、Transformer）中，ReLU显著提升训练速度。

### 改进变体
为解决ReLU的缺点，出现了一些变体：
- **Leaky ReLU**：f(x) = max(alpha x, x)，其中 alpha 是一个小的正数（如0.01），允许负输入有小梯度，缓解死神经元问题。
- **Parametric ReLU（PReLU）**：alpha 作为可学习的参数。
- **ELU（Exponential Linear Unit）**：对负输入使用指数函数，平滑过渡并具有零中心特性。
- **GELU（Gaussian Error Linear Unit）**：结合高斯分布特性，用于Transformer模型。

### 总结
ReLU因其简单、高效和缓解梯度消失的能力，成为深度学习中的首选激活函数，尤其适合隐藏层。尽管存在死神经元等局限性，其变体（如Leaky ReLU）可进一步优化性能。在实际应用中，ReLU通常是构建深层网络的起点选择。

## softmax激活函数
Softmax激活函数是一种常用的非线性激活函数，广泛应用于多分类问题中，尤其是在神经网络的输出层。以下是其简介：

### 定义
<img width="1378" height="292" alt="image" src="https://github.com/user-attachments/assets/bef34a8f-507c-4306-8774-9a413a002c01" />
  

### 特点
1. **输出范围**：每个输出值在 [0, 1]，且总和为1，适合表示多分类任务中的概率分布。
2. **非线性**：通过指数运算引入非线性，放大输入之间的差异。
3. **相对性**：Softmax的输出不仅依赖于某个输入值，还受其他输入值的影响，强调最大值的突出性。

### 优点
1. **概率解释**：输出可以直接解释为每个类别的概率，便于多分类任务的预测和评估。
2. **与交叉熵损失兼容**：Softmax常与交叉熵损失函数结合使用，优化多分类问题的训练。
3. **放大差异**：指数运算使较大的输入值在输出中占据更大比例，增强模型对最可能类别的信心。

### 缺点
1. **计算开销**：指数运算和归一化步骤相比ReLU等简单激活函数计算成本较高。
2. **梯度消失风险**：当输入值差异较大时，较小的输入对应的输出接近0，梯度可能变得极小，影响训练。
3. **不适合隐藏层**：由于其归一化特性，Softmax通常仅用于输出层，而不适合隐藏层（隐藏层更常用ReLU或tanh）。

### 应用场景
- **多分类问题**：Softmax常用于神经网络输出层，如图像分类（e.g., MNIST、ImageNet）或自然语言处理中的文本分类。
- **概率输出**：当需要模型输出每个类别的概率时，Softmax是标准选择。
- **结合交叉熵损失**：在深度学习框架中，Softmax通常与交叉熵损失集成，简化多分类任务的训练。

### 与其他激活函数的对比
- **Sigmoid**：Sigmoid用于二分类或独立概率输出（多标签分类），而Softmax用于多分类，输出归一化为概率分布。
- **ReLU**：ReLU适合隐藏层，计算简单但不提供概率输出；Softmax专为输出层设计。
- **tanh**：tanh输出范围为 [-1, 1]，不适合直接表示概率，而Softmax专为概率分布设计。

### 注意事项
<img width="1311" height="166" alt="image" src="https://github.com/user-attachments/assets/96735816-7e56-4cb1-81e7-94c41912a7f1" />
  

### 总结
Softmax激活函数是多分类任务中的核心组件，通过将输入转化为概率分布，为模型提供直观的类别预测。其与交叉熵损失的结合使其在深度学习中广泛应用，尤其在输出层。尽管计算成本较高且不适合隐藏层，Softmax在多分类场景中的表现使其成为不可或缺的激活函数。

## tanh激活函数
**tanh激活函数简介**

tanh（双曲正切）激活函数是一种常用的非线性激活函数，广泛应用于神经网络中，尤其在早期循环神经网络（如RNN、LSTM）中常见。以下是其简介：

### 定义
<img width="889" height="412" alt="image" src="https://github.com/user-attachments/assets/e7ca6df9-545e-4755-9bc9-d9ac1fb6c71e" />
  

### 特点
1. **输出范围**：tanh将任意实数输入映射到 (-1, 1) 区间，输出以0为中心。
2. **非线性**：通过双曲正切运算引入非线性，帮助神经网络建模复杂关系。
3. **平滑性**：函数连续且可导，适合基于梯度的优化方法。
<img width="1013" height="60" alt="image" src="https://github.com/user-attachments/assets/1677e56c-c618-4849-a20f-d7a9a94d216b" />  
  

### 优点
1. **零中心输出**：相比Sigmoid（输出范围[0, 1]，tanh的输出以0为中心，有助于平衡梯度更新，加速收敛。
2. **强非线性**：tanh的S形曲线比ReLU更平滑，适合需要平滑过渡的场景。
3. **适合RNN**：在传统循环神经网络（如LSTM、GRU）中，tanh常用于门控机制或隐藏状态，因其输出范围和梯度特性适合序列建模。

### 缺点
1. **梯度消失**：当输入值较大或较小时，tanh的梯度接近0，导致深层网络或长序列训练时梯度消失，影响学习效率。
2. **计算复杂性**：相比ReLU，tanh涉及指数运算，计算成本较高。
3. **不适合深层网络隐藏层**：由于梯度消失问题，tanh在现代深层网络（如CNN）中通常被ReLU及其变体取代。

### 应用场景
- **循环神经网络（RNN/LSTM/GRU）**：tanh常用于生成隐藏状态或门控机制（如LSTM的输出门），因其零中心特性适合序列建模。
- **浅层网络**：在较小的网络中，tanh可作为隐藏层的激活函数，提供非线性。
- **回归任务**：当输出需要归一化到 [-1, 1] 范围时，tanh是合适选择。

### 与其他激活函数的对比
- **Sigmoid**：Sigmoid输出范围为 [0, 1]，非零中心，可能导致梯度更新不平衡；tanh输出 [-1, 1]，更适合隐藏层。
- **ReLU**：ReLU计算简单，缓解梯度消失，但在负输入区域输出为0，可能导致死神经元；tanh输出连续但易梯度消失。
- **Softmax**：Softmax用于多分类输出层，生成概率分布；tanh更适合隐藏层或回归任务。

### 注意事项
- **梯度消失**：在深层网络或长序列任务中，tanh可能导致训练困难，需结合梯度裁剪或更现代的激活函数（如ReLU）。
- **数值稳定性**：tanh的指数运算可能引发数值溢出问题，但现代框架通常优化了实现。

### 总结
tanh激活函数因其零中心输出和平滑非线性特性，在早期神经网络和循环网络中广泛使用。尽管在深层网络中因梯度消失问题逐渐被ReLU等替代，但在特定场景（如RNN或需要 \([-1, 1]\) 输出的任务）中仍具有重要价值。其简单导数和对称性使其在某些架构中依然是可靠选择。
