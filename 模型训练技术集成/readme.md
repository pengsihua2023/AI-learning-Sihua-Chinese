## 深度学习模型训练技术集成
- [梯度裁剪（clip_grad_norm_)](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA.md)
- [学习率调度（ReduceLROnPlateau）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6.md)
- 训练早停(Early Stopping)
- 随机丢弃神经元 (Dropout)
- 标准化批次输入 (Batch Normalization) 
- 标准化层输入 (Layer Normalization)
- L1范数正则化 (L1 Regularization)
- L2范数正则化 （L2 Regularization (Weight Decay)）
- 添加噪声到输入/权重 （Noise Injection）
- 动态调整学习率 （Learning Rate Scheduling）
- 根据损失监控自动降低学习率 （ReduceLROnPlateau）
- 自适应学习率 （Adam Optimizer）
- 自适应学习率 （RMSprop）
- Adam变体 （AdamW）
- 均匀/正态分布初始化 （Xavier/Glorot Initialization）
- 考虑ReLU的方差的初始化 （He Initialization）
- 均匀分布初始化 （Uniform Initialization）
- 正态分布初始化 （Normal Initialization）
- He初始化的均匀变体 （Kaiming Uniform）
- 层级标准化初始化 （LSUV (Layer-Sequential Unit-Variance)）
- 零初始化 （Zero Initialization）
- 累积梯度 （Gradient Accumulation）
- 使用float16加速 （Mixed Precision Training）
- 多GPU并行训练 （Distributed Data Parallel (DDP)）
- 少参数微调 （LoRA）
- 量化+LoRA （QLoRA）
- 重启LoRA （ReLoRA）
- 添加小型适配器层 （Adapter Modules）
- 微调提示嵌入 （Prompt Tuning）
- 差异化剪枝 （Diff Pruning）
- 模型剪枝（移除不重要权重, Model Pruning）
- 高效注意力计算 （Flash Attention）
- 超参数搜索 （Bayesian Optimization）
- 多个模型集成 （Ensemble Learning）
