## 学习率调度
学习率调度（Learning Rate Scheduling）是深度学习训练中一种动态调整学习率（Learning Rate）的技术。学习率决定了模型参数在梯度下降过程中更新的步长，合适的调度策略可以在训练初期快速收敛，后期精细调整，从而提高模型性能并避免震荡或陷入局部最优。

常见的学习率调度方法包括：
1. **固定学习率**：整个训练过程使用恒定的学习率（不属于调度，但作为对比）。
2. **时间衰减（Step Decay）**：每隔固定轮数（epoch）将学习率降低一定比例。
3. **指数衰减（Exponential Decay）**：学习率随时间呈指数下降。
4. **余弦退火（Cosine Annealing）**：学习率按照余弦函数规律变化，平滑调整。
5. **自适应调度**：如 ReduceLROnPlateau，根据验证损失调整学习率。

学习率调度通常与优化器（如SGD、Adam）结合使用，广泛应用于深度学习框架如PyTorch和TensorFlow。

---

### Python代码示例（基于PyTorch）

以下示例展示如何在PyTorch中使用几种常见学习率调度方法，包括**时间衰减**、**指数衰减**和**余弦退火**。

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR, ExponentialLR, CosineAnnealingLR
import matplotlib.pyplot as plt

# 定义一个简单的全连接神经网络
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(10, 5)
        self.fc2 = nn.Linear(5, 2)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# 超参数
input_size = 10
batch_size = 32
epochs = 20
initial_lr = 0.01

# 初始化模型、损失函数和优化器
model = SimpleNet()
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=initial_lr)

# 模拟输入数据和标签
inputs = torch.randn(batch_size, input_size)
targets = torch.randn(batch_size, 2)

# 定义不同的学习率调度器
schedulers = {
    "StepLR": StepLR(optimizer, step_size=5, gamma=0.1),  # 每5个epoch，学习率乘以0.1
    "ExponentialLR": ExponentialLR(optimizer, gamma=0.95),  # 每epoch学习率乘以0.95
    "CosineAnnealingLR": CosineAnnealingLR(optimizer, T_max=epochs)  # 余弦退火，周期为epochs
}

# 记录学习率变化
lr_history = {name: [] for name in schedulers}

# 训练函数
def train_with_scheduler(scheduler, scheduler_name):
    current_lr_history = []
    for epoch in range(epochs):
        optimizer.zero_grad()  # 清空梯度
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        
        # 记录当前学习率
        current_lr = optimizer.param_groups[0]['lr']
        current_lr_history.append(current_lr)
        
        # 更新学习率
        scheduler.step()
        
        print(f"[{scheduler_name}] Epoch {epoch+1}, Loss: {loss.item():.4f}, LR: {current_lr:.6f}")
    
    return current_lr_history

# 运行训练并记录学习率
for name, scheduler in schedulers.items():
    print(f"\nRunning with {name}")
    # 重置优化器以确保公平比较
    optimizer = optim.SGD(model.parameters(), lr=initial_lr)
    lr_history[name] = train_with_scheduler(scheduler, name)

# 可视化学习率变化
plt.figure(figsize=(10, 6))
for name, lrs in lr_history.items():
    plt.plot(range(epochs), lrs, label=name)
plt.xlabel("Epoch")
plt.ylabel("Learning Rate")
plt.title("Learning Rate Schedules")
plt.legend()
plt.grid(True)
plt.show()
```

---

### 代码说明

1. **模型和优化器**：
   - 定义了一个简单的全连接神经网络 `SimpleNet`。
   - 使用SGD优化器，初始学习率为0.01。

2. **学习率调度器**：
   - **StepLR**：每5个epoch，学习率乘以0.1（`gamma=0.1`）。
   - **ExponentialLR**：每个epoch学习率乘以0.95（`gamma=0.95`）。
   - **CosineAnnealingLR**：学习率按余弦函数变化，周期为整个训练轮数（`T_max=epochs`）。

3. **训练过程**：
   - 每个epoch计算损失、反向传播、更新参数。
   - 使用 `scheduler.step()` 更新学习率。
   - 记录每个epoch的学习率以便后续可视化。

4. **可视化**：
   - 使用 `matplotlib` 绘制不同调度器的学习率变化曲线。

---

### 输出与可视化

运行代码后，控制台会输出每个epoch的损失和当前学习率，例如：
```
[StepLR] Epoch 1, Loss: 1.2345, LR: 0.010000
[StepLR] Epoch 6, Loss: 0.9876, LR: 0.001000
...
[ExponentialLR] Epoch 1, Loss: 1.2345, LR: 0.010000
[ExponentialLR] Epoch 2, Loss: 1.1234, LR: 0.009500
...
[CosineAnnealingLR] Epoch 1, Loss: 1.2345, LR: 0.010000
...
```

最终会生成一张图，展示三种调度器的学习率变化趋势：
- **StepLR**：学习率每5个epoch阶梯式下降。
- **ExponentialLR**：学习率平滑指数衰减。
- **CosineAnnealingLR**：学习率呈余弦波形变化。

---

### 实际应用场景

- **StepLR**：适合需要快速降低学习率的场景，如CNN训练。
- **ExponentialLR**：适合需要平滑衰减的场景，如RNN或LSTM。
- **CosineAnnealingLR**：常用于现代深度学习模型（如Transformer），因其平滑变化能帮助模型更好地探索损失平面。
- **ReduceLROnPlateau**（未在代码中展示）：当验证损失停止下降时降低学习率，适合需要动态调整的任务。

---

### 补充：ReduceLROnPlateau 示例

如果需要实现自适应调度，可以添加以下代码：

```python
from torch.optim.lr_scheduler import ReduceLROnPlateau

# 定义调度器
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)

# 模拟验证损失
val_loss = 1.0
for epoch in range(epochs):
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()
    
    # 假设验证损失逐渐减小
    val_loss = max(0.1, val_loss * 0.95)
    scheduler.step(val_loss)
    
    print(f"[ReduceLROnPlateau] Epoch {epoch+1}, Val Loss: {val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}")
```

---
