
# 散度的基础知识

## 目录

1. [什么是散度？](#1-什么是散度)

   * (1) Kullback–Leibler 散度
   * (2) Jensen–Shannon 散度
   * (3) Wasserstein 距离
2. [散度在深度学习中的应用](#2-散度在深度学习中的应用)

   * (1) 损失函数
   * (2) 生成模型
   * (3) 分布匹配
   * (4) 强化学习
3. [散度比较](#3-散度比较)

---

## 1. 什么是散度？

在数学和信息论中，**散度** 通常指的是两个概率分布之间的差异度量。

### (1) Kullback–Leibler 散度 (KL Divergence)

离散型：

$D_{\mathrm{KL}}(P \parallel Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}$

连续型：

$D_{\mathrm{KL}}(P \parallel Q) = \int P(x) \log \frac{P(x)}{Q(x)} \, dx$

* **$P(x)$**：真实分布（target / data distribution），表示在事件 $x$ 上的真实概率。
* **$Q(x)$**：近似分布或模型分布（approximation / model distribution），表示在事件 $x$ 上模型的估计概率。
* **解释**： $D_{\mathrm{KL}}(P\|Q)$ 衡量在用 $Q$ 来近似 $P$ 时，信息损失的大小。

---

### (2) Jensen–Shannon 散度 (JS Divergence)

$D_{\mathrm{JS}}(P \Vert Q) = \tfrac{1}{2} D_{\mathrm{KL}}(P \Vert M) + \tfrac{1}{2} D_{\mathrm{KL}}(Q \Vert M), \quad M = \tfrac{1}{2}(P+Q)$

* **$P, Q$**：两个概率分布。
* **$M = \frac{1}{2}(P+Q)$**：混合分布，即对两个分布取平均。
* **解释**：JS 散度是基于 KL 散度的对称化版本，保证了**对称性**（即 $D_{JS}(P\|Q) = D_{JS}(Q\|P)$）和**有界性**（取值在 \[0, 1] 之间，若使用 log base 2）。

**特点：** 对称、有界。

---

### (3) Wasserstein 距离 (Earth Mover’s Distance)

描述为“将一个分布转化为另一个分布所需的最小搬运代价”。常用于生成对抗网络 (WGAN)。

$W(P, Q) = \inf_{\gamma \in \Pi(P,Q)} \mathbb{E}_{(x,y) \sim \gamma} \big[ \lVert x - y \rVert \big]$

* **$P, Q$**：两个概率分布。
* **$\Pi(P, Q)$**：所有可能的联合分布（couplings），边缘分布分别为 $P$ 和 $Q$。
* **$E(x,y) \sim \gamma$**：表示在联合分布 $\gamma$ 下的期望。
* **$\|x-y\|$**：从点 $x$ 移动到点 $y$ 的“距离”（通常是欧氏距离）。
* **解释**：Wasserstein 距离衡量将一个分布“搬运”成另一个分布所需的最小代价。

---

### 小结
* **KL 散度**：不对称，衡量分布差异，常用于信息论与概率建模。
* **JS 散度**：对称化后的 KL 散度，常用于比较两个分布的相似性。
* **Wasserstein 距离**：考虑“搬运代价”的分布差异度量，常用于生成对抗网络（WGAN）。

## 2. 散度在深度学习中的应用

### (1) 损失函数

交叉熵损失：

$H(P,Q) = H(P) + D_{\mathrm{KL}}(P \parallel Q)$

---

### (2) 生成模型

**变分自编码器 (VAE)：**


<div align="left">
  
$\mathcal{L_VAE} \= \mathcal{E_{q_\phi(z \mid x)}} \left[ \log p_\theta(x \mid z) \right] - D_{\mathrm{KL}}\left( q_\phi(z \mid x) \Vert p(z) \right) $

</div>

**生成对抗网络 (GAN)：**

* 原始 GAN：最小化 JS 散度
* WGAN：最小化 Wasserstein 距离

---

### (3) 分布匹配

知识蒸馏 (Knowledge Distillation)：

$\min_\theta D_{\mathrm{KL}}(P \parallel Q)$

其中 \$P\$ 是教师分布， \$Q\$  是学生分布。

---

### (4) 强化学习

在策略优化方法（如 TRPO、PPO）中，KL 散度常用于约束新旧策略的差异：

$D_{\mathrm{KL}}(\pi_{\text{old}} \parallel \pi_{\text{new}}) \leq \delta$

---

## 3. 散度比较

| 散度                        | 公式                                                                                                                                                 | 特性                       | 优点               | 缺点                                | 应用                   |
| ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------ | ---------------- | --------------------------------- | -------------------- |
| **Kullback–Leibler (KL)** | $D_{\mathrm{KL}}(P \Vert Q) = \sum_x P(x)\log\frac{P(x)}{Q(x)}$                                                                                | 非负、不对称；当 $P=Q$ 时为 0      | 直观、易于计算；与交叉熵紧密相关 | 如果 $Q(x)=0$ 且 $P(x)>0$，结果为无穷大；不对称 | 分类任务（交叉熵损失）、VAE、知识蒸馏 |
| **Jensen–Shannon (JS)**   | $D_{\mathrm{JS}}(P \Vert Q) = \tfrac{1}{2}D_{\mathrm{KL}}(P \Vert M) + \tfrac{1}{2}D_{\mathrm{KL}}(Q \Vert M) \; M=\tfrac{1}{2}(P+Q)$ | 对称、有界（在 0 和 $\log 2$ 之间） | 对称性好、稳定性强        | 当分布无重叠时，梯度消失                      | 原始 GAN               |
| **Wasserstein 距离**        | $W(P,Q)=\inf_{\gamma \in \Pi(P,Q)} \mathbb{E}_{(x,y)\sim\gamma}\left[\lVert x-y\rVert\right]$                                                      | 满足度量性质；反映分布的几何差异         | 即使分布不重叠也能提供平滑梯度  | 计算复杂，需要最优传输理论                     | WGAN、分布对齐            |

---


