
# 散度及其在深度学习中的应用

## 1. 什么是散度？

在数学和信息论中，**散度 (Divergence)** 通常指的是两个概率分布之间的差异度量。

### (1) Kullback–Leibler 散度 (KL Divergence)

离散型：


$$
D_{\mathrm{KL}}(P \parallel Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
$$


连续型：


$$
D_{\mathrm{KL}}(P \parallel Q) = \int P(x) \log \frac{P(x)}{Q(x)} \, dx
$$


### (2) Jensen–Shannon 散度 (JS Divergence)


$$
D_{\mathrm{JS}}(P \parallel Q) = \tfrac{1}{2} D_{\mathrm{KL}}(P \parallel M) + \tfrac{1}{2} 

$$
D_{\mathrm{KL}}(Q \parallel M), \quad M = \tfrac{1}{2}(P+Q)
$$

特点：对称、取值有限。  

### (3) Wasserstein 距离 (Earth Mover’s Distance)

描述为“将一个分布变成另一个分布所需的最小搬运代价”。常见于生成对抗网络 (WGAN) 中。

$$
W(P, Q) = \inf_{\gamma \in \Pi(P,Q)} \mathbb{E}_{(x,y) \sim \gamma} \big[ \lVert x - y \rVert \big]
$$

## 2. 散度在深度学习中的应用

### (1) 损失函数 (Loss Function)

交叉熵损失：


$$
H(P,Q) = H(P) + D_{\mathrm{KL}}(P \parallel Q)
$$

### (2) 生成模型 (Generative Models)

**变分自编码器 (VAE):**
<img width="681" height="71" alt="image" src="https://github.com/user-attachments/assets/245e5754-f5a4-4428-bdd2-8c6b8b277ba8" />


$$\mathcal{L_VAE} $$
$$\= \mathbb{E}_{q_\phi(z \mid x)} \left[ \log p_\theta(x \mid z) \right] - 

$$
D_{\mathrm{KL}}\!\left( q_\phi(z \mid x) \,\Vert\, p(z) \right)
$$



**生成对抗网络 (GAN):**

* 原始 GAN：最小化 JS 散度
* WGAN：最小化 Wasserstein 距离



### (3) 分布匹配 (Distribution Matching)

知识蒸馏 (Knowledge Distillation)：


$$
\min_\theta D_{\mathrm{KL}}(P \parallel Q)
$$


其中 $P$ 为教师网络分布，$Q$ 为学生网络分布。



### (4) 强化学习 (Reinforcement Learning)

策略优化方法（如 TRPO, PPO）中，常用 KL 散度来约束新旧策略的差异：


$$
D_{\mathrm{KL}}(\pi_{\text{old}} \parallel \pi_{\text{new}}) \leq \delta
$$




