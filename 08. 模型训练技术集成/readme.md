## 深度学习模型训练技术集成
- 关键技术和诀窍
  - Create 自动混合精度（AMP）
  - Curriculum Learning
  - Optuna超参数优化方法
  - Ray Tune超参数优化方法
  - 处理类别不平衡处理方法
  - Min-Max 归一化
  - Z-score 标准化
  - Layer Normalization
  - Batch Normalization
  - 使用float16加速 （Mixed Precision Training）
  - 多GPU并行训练 （Distributed Data Parallel (DDP)）
  - [梯度裁剪（clip_grad_norm_)]()
  - [累积梯度 （Gradient Accumulation）]()
  - [高效注意力计算 （Flash Attention）]()
  - [超参数搜索 （Bayesian Optimization）]()
  - 多个模型集成 （Ensemble Learning） 
- [正则化技术概述](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF%E6%A6%82%E8%BF%B0.md)
  - [L1范数正则化 (L1 Regularization)](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/L1%E8%8C%83%E6%95%B0%E6%AD%A3%E5%88%99%E5%8C%96.md)
  - [L2范数正则化 （L2 Regularization (Weight Decay)）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/L2%E8%8C%83%E6%95%B0%E6%AD%A3%E5%88%99%E5%8C%96.md)  - 
  - [标准化批次输入 (Batch Normalization)]()
  -  [标准化层输入 (Layer Normalization)]()
  -  [训练早停(Early Stopping)](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%AD%A3%E5%88%99%E5%8C%96%E6%8A%80%E6%9C%AF%E4%B8%93%E9%A2%98/%E8%AE%AD%E7%BB%83%E6%97%A9%E5%81%9C.md)
  -  [添加噪声到输入/权重 （Noise Injection）]()
  -  [随机丢弃神经元 (Dropout)]()
- [学习率调整方法概述](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0.md)
  - [动态调整学习率 （Learning Rate Scheduling）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E5%8A%A8%E6%80%81%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87%EF%BC%88Learning%20Rate%20Scheduling%EF%BC%89.md)
  - [根据损失监控自动降低学习率 （ReduceLROnPlateau）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E6%A0%B9%E6%8D%AE%E6%8D%9F%E5%A4%B1%E7%9B%91%E6%8E%A7%E8%87%AA%E5%8A%A8%E9%99%8D%E4%BD%8E%E5%AD%A6%E4%B9%A0%E7%8E%87%EF%BC%88ReduceLROnPlateau%EF%BC%89.md)
  - [自适应学习率 （Adam Optimizer）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87%20%EF%BC%88Adam%20Optimizer%EF%BC%89.md)
  - [自适应学习率 （RMSprop）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87%20%EF%BC%88RMSprop%EF%BC%89.md)
- [优化器概述](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%93%E9%A2%98/%E4%BC%98%E5%8C%96%E5%99%A8%E6%A6%82%E8%BF%B0.md)
  - [Adam优化器](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%93%E9%A2%98/Adam%20Optimizer.md)
  - [Adam变体 （AdamW）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%93%E9%A2%98/Adam%E5%8F%98%E4%BD%93%20%EF%BC%88AdamW%EF%BC%89.md)
  - [SGD优化器（随机梯度下降，Stochastic Gradient Descent）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%93%E9%A2%98/SGD%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%88%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%8CStochastic%20Gradient%20Descent%EF%BC%89.md)
  - [RMSProp优化器（Root Mean Square Propagation）]()
  - [Adagrad优化器（Adaptive Gradient Algorithm）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%93%E9%A2%98/Adagrad%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%88Adaptive%20Gradient%20Algorithm%EF%BC%89.md)
  - [Adadelta优化器](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%93%E9%A2%98/Adadelta%E4%BC%98%E5%8C%96%E5%99%A8.md)  
  - [AMSGrad优化器](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%93%E9%A2%98/AMSGrad%E4%BC%98%E5%8C%96%E5%99%A8.md)
  - [Nadam优化器]
  - [L-BFGS优化器（Limited-memory Broyden–Fletcher–Goldfarb–Shanno）]
  - [Rprop优化器（Resilient Backpropagation）]
  - [SparseAdam优化器]
  - [ASGD优化器（Averaged Stochastic Gradient Descent）]
- [初始化方法概述](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0.md)
  - [均匀/正态分布初始化 （Xavier/Glorot Initialization](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E5%9D%87%E5%8C%80-%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E5%88%9D%E5%A7%8B%E5%8C%96%20%EF%BC%88Xavier-Glorot%20Initialization%EF%BC%89.md)
  - 考虑ReLU的方差的初始化 （He Initialization）
  - 均匀分布初始化 （Uniform Initialization）
  - [正态分布初始化 （Normal Initialization）](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E6%B3%95%E4%B8%93%E9%A2%98/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E5%88%9D%E5%A7%8B%E5%8C%96%20%EF%BC%88Normal%20Initialization%EF%BC%89.md)
  - He初始化的均匀变体 （Kaiming Uniform）
  - 层级标准化初始化 （LSUV (Layer-Sequential Unit-Variance)）
  - 零初始化 （Zero Initialization）
- 模型微调技术概述
  - 少参数微调 （LoRA）
  - 微调提示嵌入 （Prompt Tuning）
  - 重启LoRA （ReLoRA）
  - 量化+LoRA （QLoRA）
  - 差异化剪枝 （Diff Pruning）
  - 添加小型适配器层 （Adapter Modules）
  - 模型剪枝（移除不重要权重, Model Pruning）
- [深度学习模型评估方法汇总](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%E6%B1%87%E6%80%BB.md)
- [深度学习模型解释方法概述](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes/blob/main/08.%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E9%9B%86%E6%88%90/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A7%A3%E9%87%8A%E5%B7%A5%E5%85%B7%E6%B1%87%E6%80%BB.md)
  - SHAP
  - LIME
  - Saliency Maps
  - Integrated Gradients
  - Grad-CAM
  - DeepLIFT
  - t-SNE 
  - Attention Visualization

